version: "3.9"

networks:
  spark-net:
    driver: bridge

services:
  # ------------------- Hadoop NameNode -------------------
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    hostname: hadoop-namenode
    platform: linux/amd64  #兼容mac
    volumes:
      - ./hadoop-data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_replication=1
    ports:
      - "9870:9870"
      - "8020:8020"
    networks:
      - spark-net

  # ------------------- Hadoop DataNode -------------------
  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    hostname: hadoop-datanode
    platform: linux/amd64
    depends_on:
      - hadoop-namenode
    volumes:
      - ./hadoop-data/datanode:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data
      - SERVICE_PRECONDITION=hadoop-namenode:8020
    networks:
      - spark-net

  # ------------------- Spark Master (使用官方最新镜像) -------------------
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    hostname: spark-master
    command: /opt/spark/sbin/start-master.sh
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_MASTER_PORT=7077
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:/opt/spark-events
      - HADOOP_CONF_DIR=/opt/hadoop-conf
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    volumes:
      - ./spark-events:/opt/spark-events
      - ./hadoop-conf:/opt/hadoop-conf
    networks:
      - spark-net

  # ------------------- Spark Worker 1 -------------------
  spark-worker-1:
    image: apache/spark:3.5.1
    container_name: spark-worker-1
    hostname: spark-worker-1
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
      - HADOOP_CONF_DIR=/opt/hadoop-conf
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:/opt/spark-events
    depends_on:
      - spark-master
    volumes:
      - ./spark-events:/opt/spark-events
      - ./hadoop-conf:/opt/hadoop-conf
    networks:
      - spark-net

  # ------------------- Spark Worker 2 -------------------
  spark-worker-2:
    image: apache/spark:3.5.1
    container_name: spark-worker-2
    hostname: spark-worker-2
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
      - HADOOP_CONF_DIR=/opt/hadoop-conf
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:/opt/spark-events
    depends_on:
      - spark-master
    volumes:
      - ./spark-events:/opt/spark-events
      - ./hadoop-conf:/opt/hadoop-conf
    networks:
      - spark-net

  # ------------------- Spark History Server -------------------
  spark-history:
    image: apache/spark:3.5.1
    container_name: spark-history
    hostname: spark-history
    command: /opt/spark/sbin/start-history-server.sh
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:/opt/spark-events
      - SPARK_HISTORY_UI_PORT=18080
      - HADOOP_CONF_DIR=/opt/hadoop-conf
    ports:
      - "18080:18080"
    volumes:
      - ./spark-events:/opt/spark-events
      - ./hadoop-conf:/opt/hadoop-conf
    networks:
      - spark-net